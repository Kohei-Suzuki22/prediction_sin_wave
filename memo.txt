ampとは？
  → noiseに関係している。ノイズの規模を定めるもの。

epochとは？
  → どれくらい繰り返し学習させるか。

linear関数とは？
  → 変換しない活性化関数。
  ・ 重みWが掛け算されるため、入力刺激値に比例した値が伝わる。

BPTT(Back Propagation Through Time)とは？

  ・Back Propagation(誤差逆伝搬法)に時間の概念を追加する。
  ・つまり、誤差を求めるパラメーターに時間変化も加味される。


RNNとは？
  → 順序に意味のあるデータ(時系列)を学習させるのに適したニューラルネットワーク。

  ・ニューロンの出力を自分の次の入力に対して処理していく。

  ex) 1~10, 2~11, 3~12, ・・・のように一個ずつずらしたデータ群を入力として学習。

  ■ RNNの種類
    ・elman_net(SimpleRNN): 長期間にわたる時系列データを扱うのは苦手。
    ・LSTM: 長期の時系列にも短期の時系列にも対応出来る。学習時間が大。
    ・GRU:  長期の時系列にも短期の時系列にも対応出来る。LSTMよりもシンプルで学習時間が少。


SimpleRNN(単純再起型ニューラルネットワーク)
  → 時系列データを学習できる。
  → 出力が入力にフィードバックをされる全結合RNN

バッチサイズとは？

エポックとは？
  → 学習を繰り返す回数。


過学習とは？
  → 訓練用に与えた学習データ専用のフィッティングになること。
  → 学習データに対しては抜群の精度のフィッティングをするが、
    本番データに対しての挙動がおかしくなること。


■ 活性化関数の検証(kerasで指定できるもの。)

  検証対象
    ・softmax         → ×
    ・elu             → 〇
    ・selu            → 〇
    ・softplus        → ×
    ・softsign        → ×
    ・relu            → ×
    ・tanh            → ×
    ・sigmoid         → ×
    ・hard_sigmoid    → ×
    ・linear          → 〇

  ・elu, selu, linear で多少違いがあるが、どれも近い値を示す。

■ 最適化アルゴリズム(最適化手法)について

  ・勾配降下法と最急降下法の違いは？
  ・勾配降下法も最急降下法も、微分による極小値が複数ある場合に、困る。

  → これを解決するのが、SGD(=確率的勾配降下法): ランダム性を含んだ再急降下法。





■ 結果
  ・affect_length(過去のデータを考慮する数)は、多すぎても少なすぎてもよくなかった。
      → 一番適切な予測が出来たのが、affect_length = 32 の場合であった。
  ・